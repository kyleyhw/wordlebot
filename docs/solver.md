# Information Theory Solver (`solver.py`)

## 1. Purpose

The `solver.py` module implements an optimal Wordle solver based on an information theory approach. Its primary goal is to minimize the number of guesses required to solve a Wordle puzzle by selecting guesses that provide the maximum amount of information at each step. The solver is designed to be independent of the game's UI and can be used for simulations or to suggest optimal moves in an interactive game.

## 2. Mathematical Principles: Information Theory and Optimization Metrics

The core idea behind this solver is to leverage information theory, specifically the concept of **entropy** [[1]](#ref-shannon-1948), to quantify the "informativeness" of a guess. In the context of Wordle, a guess is more informative if it effectively narrows down the set of possible solution words.

### Entropy Calculation (for `search_depth=1` and `min_avg_remaining` optimization)

For a given guess `w` and a set of `N` currently possible solution words, the entropy $H(w)$ is calculated as follows:

$H(w) = -\sum_{p \in \text{Patterns}} P(p|w) \log_2 P(p|w)$

Where:
*   $P(p|w)$ is the probability of observing a specific feedback pattern $p$ (e.g., 'gybby') if we make guess $w$.
*   $P(p|w)$ is calculated as $\frac{\text{count}(p)}{N}$, where $\text{count}(p)$ is the number of possible solution words that would produce pattern $p$ when compared against guess `w`.
*   The sum is taken over all unique feedback patterns that can be generated by guess $w$ against the current set of possible solutions.

A higher entropy value indicates that the guess is expected to split the remaining possible solutions into more, smaller, and more evenly distributed groups, thus providing more information and reducing uncertainty more effectively.

### Optimization Metrics for Multi-Layer Search

For multi-layer searches (`search_depth > 1`), the solver can optimize for different metrics:

*   **`min_max_remaining` (Minimize Maximum Remaining Solutions):**
    *   **Goal**: To choose a guess that guarantees the *best worst-case scenario*. That is, it minimizes the size of the *largest* possible group of remaining solutions after the guess, considering future steps up to `search_depth`.
    *   **Rationale**: This strategy aims to provide a strong guarantee against very bad outcomes, ensuring that no matter what feedback is received, the solver is never left with an overwhelmingly large number of possible solutions.

*   **`min_avg_remaining` (Minimize Average Remaining Solutions):**
    *   **Goal**: To choose a guess that, on average, reduces the size of the possible solution set the most, considering future steps up to `search_depth`.
    *   **Rationale**: This metric is closely related to entropy and aims for the best overall performance by creating many small, evenly sized groups of solutions.

*   **`min_avg_guesses` (Minimize Average Future Guesses):**
    *   **Goal**: To choose a guess that minimizes the expected number of *additional* guesses required to solve the puzzle from the current state, considering future steps up to `search_depth`.
    *   **Algorithm (Minimax Expected Guesses)** [[2]](#ref-knuth-1975):
        This metric is significantly more complex and computationally intensive than the others, as it involves estimating the full game tree from each state. The algorithm is recursive and relies heavily on **memoization** to be computationally feasible.

        1.  **Base Case**: If there is only one possible solution left, it takes 1 guess (the current one). If there are no solutions, it's an error state (or effectively infinite guesses).
        2.  **Recursive Step**: For a given set of `current_possible_solutions` and a `guess`:
            a.  Simulate all possible feedback patterns that `guess` could produce against `current_possible_solutions`.
            b.  For each feedback pattern `p`:
                i.  Filter `current_possible_solutions` to get `next_possible_solutions` (the subset of words that would produce `p`).
                ii. If `next_possible_solutions` contains only one word, the expected future guesses for this branch is 1 (the current guess). This is because if we make the current guess and get feedback `p`, we know the word, and it takes 1 more guess to get it.
                iii. Otherwise, recursively call the `_calculate_expected_guesses` function for `next_possible_solutions` (with `depth - 1`) to find the *optimal next guess* and its *expected future guesses*. Let this be `E_p`.
                iv. The contribution of this branch to the total expected guesses for the current `guess` is `(1 + E_p) * P(p)`, where `P(p)` is the probability of getting feedback `p` (i.e., `len(next_possible_solutions) / len(current_possible_solutions)`).
            c.  Sum up the contributions from all feedback patterns to get the total expected future guesses for the current `guess`.
        3.  **Selection**: Choose the `guess` that minimizes this total expected future guesses.

    *   **Computational Challenges**: Even with memoization, the computation for deeper searches is immense. The number of unique subsets of `possible_solutions` can be very large, leading to very long computation times, especially during the initial guess pre-calculation. A `search_depth` greater than 2 or 3 can quickly become intractable for the full Wordle dictionary.

### What is Memoization?

**Memoization** is an optimization technique used to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. It's a form of caching.

In the context of the `_calculate_expected_guesses` function, memoization is critical because the function is recursive and often evaluates the *same set of possible solutions* multiple times through different paths in the search tree. By storing the computed expected guesses for each unique set of solutions (represented as a `frozenset` for hashability), redundant computations are avoided, dramatically reducing the overall computation time. Without memoization, the recursive calls would lead to an exponential explosion of computations.

## 3. Pre-calculation Strategy

To optimize performance, especially for the initial guess and subsequent turns, a significant pre-calculation step is performed during the solver's initialization.

### Feedback Map (`feedback_map`)

*   **Rationale**: Calculating the feedback pattern for every `(guess, solution)` pair during runtime is computationally expensive. By pre-calculating and storing these patterns, we can retrieve them in constant time during the actual solving process. The decision to pre-calculate the feedback map is a classic space-time tradeoff. We use more memory to store the map, but we save a significant amount of time during the solving process. The map is also cached to a file (`feedback_map.json`) so that it only needs to be calculated once.
*   **Implementation**: A dictionary `feedback_map` is created, where keys are `(guess_word, solution_word)` tuples and values are the 5-character feedback strings (e.g., 'gybby'). This map is populated by iterating through all `allowed_guesses` and all `possible_solutions` and calling the `get_feedback` function for each pair.

### Best Initial Guesses by Configuration (`best_initial_guesses_by_config`)

*   **Rationale**: The optimal first guess can vary depending on the chosen `search_depth` and `optimization_metric`. Since the initial state is constant, these optimal first guesses can be pre-calculated and stored.
*   **Implementation**: During initialization, the solver calculates the optimal initial guess using the configured `search_depth` and `optimization_metric`. This result is stored in a dictionary `best_initial_guesses_by_config`, keyed by a tuple `(search_depth, optimization_metric)`. This allows for quick retrieval of the appropriate first guess without re-computation if the configuration is used again.

## 4. Class: `solver`

### `__init__(self, allowed_guesses_list, possible_solutions_list, search_depth=1, optimization_metric='min_avg_remaining')`

*   **Purpose**: Initializes the Wordle solver with the complete lists of allowed guesses and possible solutions, and performs all necessary pre-calculations.
*   **Parameters**:
    *   `allowed_guesses_list` (list of str): All words that can be entered as guesses.
    *   `possible_solutions_list` (list of str): All words that can be the secret solution.
    *   `search_depth` (int, optional): The number of layers to look ahead in the search tree. Defaults to 1 (greedy).
    *   `optimization_metric` (str, optional): The metric to optimize for. Can be `'min_max_remaining'`, `'min_avg_remaining'`, or `'min_avg_guesses'`. Defaults to `'min_avg_remaining'`.
*   **Implementation Details**:
    *   Stores the input word lists, `search_depth`, and `optimization_metric`.
    *   Validates the `optimization_metric`.
    *   Executes the pre-calculation of `feedback_map` (or loads it from cache).
    *   Determines and stores the optimal initial guess for the given `search_depth` and `optimization_metric` in `best_initial_guesses_by_config`.

### `_calculate_entropy(self, guess, current_possible_solutions)`

*   **Purpose**: Calculates the entropy for a specific `guess` against a given set of `current_possible_solutions`. This method is primarily used for `search_depth=1` when `optimization_metric` is related to average remaining solutions.
*   **Parameters**:
    *   `guess` (str): The word for which to calculate entropy.
    *   `current_possible_solutions` (list of str): The subset of words that are still considered potential solutions.
*   **Returns**: (float) The calculated entropy value.
*   **Implementation Details**: 
    *   Uses the pre-calculated `feedback_map` to efficiently retrieve feedback patterns for each `(guess, solution)` pair.
    *   Counts the occurrences of each feedback pattern.
    *   Applies the entropy formula using `math.log2`.

### `_calculate_expected_guesses(self, current_possible_solutions_frozenset, depth)`

*   **Purpose**: Recursively calculates the minimum expected number of *additional* guesses required to solve the puzzle from a given state (`current_possible_solutions_frozenset`), considering a specified `depth` of lookahead. This function is used when `optimization_metric` is set to `'min_avg_guesses'`.
*   **Parameters**:
    *   `current_possible_solutions_frozenset` (frozenset of str): A hashable set of words representing the current possible solutions.
    *   `depth` (int): The remaining depth of the lookahead search.
*   **Returns**: (float) The minimum expected number of additional guesses.
*   **Implementation Details**: 
    *   **Memoization**: Utilizes `self._expected_guesses_memo` to store and retrieve previously computed results for identical sets of possible solutions, preventing redundant calculations. The keys to this memoization dictionary are `frozenset` objects because they are hashable, unlike lists.
    *   **Base Cases**: 
        *   If `current_possible_solutions_frozenset` is empty, returns 0 (should not occur in a solvable game).
        *   If `current_possible_solutions_frozenset` contains only one word, returns 1 (one more guess to get the word).
        *   If `depth` is 0, it returns `len(current_possible_solutions_frozenset)` as a heuristic. A true value would require deeper pre-calculation or a full game tree, which is computationally prohibitive for this iteration.
    *   **Recursive Step**: 
        *   Iterates through `self.allowed_guesses` as candidate guesses.
        *   For each candidate guess, it simulates all possible feedback patterns and groups the `current_possible_solutions` accordingly.
        *   For each group, it recursively calls `_calculate_expected_guesses` (with `depth - 1`) to find the expected future guesses from that next state.
        *   It then calculates the weighted average of `(1 + expected_guesses_from_next_state)` across all groups to get the total expected guesses for the current candidate guess.
        *   The minimum of these total expected guesses across all candidate guesses is chosen.

### `_evaluate_guess(self, guess, current_possible_solutions_frozenset, depth, optimization_metric)`

*   **Purpose**: Recursively evaluates the "score" of a `guess` based on the chosen `optimization_metric` and `depth` of lookahead.
*   **Parameters**:
    *   `guess` (str): The word to evaluate.
    *   `current_possible_solutions_frozenset` (frozenset of str): A hashable set of words representing the current possible solutions.
    *   `depth` (int): The remaining depth of the lookahead search.
    *   `optimization_metric` (str): The metric to optimize for.
*   **Returns**: (float) The calculated score for the guess. A lower score is better.
*   **Implementation Details**: 
    *   **Base Case**: If `depth` is 0 or `current_possible_solutions_frozenset` has 1 or fewer elements, it returns `len(current_possible_solutions_frozenset)`. This serves as a heuristic for `min_avg_guesses` when the recursion depth is exhausted.
    *   **Recursive Step**: 
        *   Groups `current_possible_solutions_frozenset` by the feedback pattern generated by the `guess`.
        *   For each resulting group, it recursively calls `_find_best_guess_multi_layer` (with `depth - 1`) to determine the best next guess in that sub-state.
        *   It then calls `_evaluate_guess` again with that `next_best_guess` and the `depth - 1` to get the outcome for that branch.
        *   Aggregates these outcomes based on the `optimization_metric`:
            *   `min_max_remaining`: Returns the maximum outcome among all branches.
            *   `min_avg_remaining`: Returns the weighted average of outcomes across all branches.
            *   `min_avg_guesses`: Directly calls `_calculate_expected_guesses` for the current state.

### `_find_best_guess_multi_layer(self, current_possible_solutions_frozenset, search_depth, optimization_metric)`

*   **Purpose**: Identifies the best guess from `self.allowed_guesses` that optimizes the chosen `optimization_metric` for a given `search_depth`.
*   **Parameters**:
    *   `current_possible_solutions_frozenset` (frozenset of str): A hashable set of words representing the current possible solutions.
    *   `search_depth` (int): The depth of the lookahead search.
    *   `optimization_metric` (str): The metric to optimize for.
*   **Returns**: (str) The guess word that yields the best score.
*   **Implementation Details**: 
    *   Handles base cases where only one solution remains or no solutions are left.
    *   Iterates through `self.allowed_guesses` (or a subset for further optimization) as `candidate_guesses`.
    *   For each `guess`, it calls `_evaluate_guess` to get its score.
    *   Keeps track of the `guess` that results in the minimum score.

### `get_next_guess(self, guess_history)`

*   **Purpose**: Provides the optimal next guess based on the game's history and the solver's configuration (`search_depth`, `optimization_metric`).
*   **Parameters**:
    *   `guess_history` (list of tuples): A list where each tuple is `(guess_word, feedback_pattern)` from previous turns.
*   **Returns**: (str) The recommended next guess.
*   **Implementation Details**: 
    *   **First Guess**: If `guess_history` is empty, it retrieves the pre-calculated optimal initial guess from `best_initial_guesses_by_config`. If not found (e.g., due to a new configuration), it calculates and stores it.
    *   **Subsequent Guesses**: 
        *   Filters `self.possible_solutions` based on `guess_history` to determine `current_possible_solutions_list`.
        *   Converts `current_possible_solutions_list` to a `frozenset` (`current_possible_solutions_frozenset`) for use with memoization.
        *   If only one solution remains, it returns that solution.
        *   Otherwise, it calls `_find_best_guess_multi_layer` with the current `search_depth` and `optimization_metric` to determine the next optimal guess.
    *   Includes error handling for scenarios where no possible solutions remain.

## 5. Dependencies

*   `collections`: Specifically `collections.defaultdict` for counting pattern occurrences and grouping solutions.
*   `math`: For `math.log2` in entropy calculation.
*   `random`: Used for fallback guesses if no optimal guess can be determined.
*   `funcs.py`: Provides the `split` utility function.
*   `rules.py`: The `get_feedback` function is a static adaptation of the logic found in `rules.py`.

## References

<a id="ref-shannon-1948"></a>
[1] Shannon, C. E. (1948). A mathematical theory of communication. *Bell System Technical Journal*, *27*(3), 379-423.

<a id="ref-knuth-1975"></a>
[2] Knuth, D. E. (1975). Estimating the efficiency of backtrack programs. *Mathematics of Computation*, *29*(129), 121-136.